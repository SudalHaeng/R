# 9. 서포트벡터머신
### 1. 서론
서포트벡터머신(support vector machines, 이하 SVM) 모형은 고차원 또는 무한 차원의 공간에서 초평면(의 집합)을 찾아 이를 이용하여 분류와 회귀를 수행한다.  
SVM은 지도학습기법으로 비-중첩(non-overlapping) 분할을 제공하며 모든 속성(attributes)을 활용하는 전역적(global) 분류 모형이다.  
SVM은 최대 마진을 가지는 선형판별에 기초하며, 속성들 간의 의존성은 고려하지 않는 방법이다.  

### 2. 기초 개념
직관적으로 자료를 군집별로 가장 잘 분리하는 초평면은 가장 가까운 훈련용 자료까지의 거리(이를 마진(margin)이라 함)가 가장 큰 경우이며, 마진이 가장 큰 초평면을 분류기(classifier)로 사용할 때, 새로운 자료에 대한 오분류가 가장 낮아진다.  
서포트벡터머신 모형은 선형분류 뿐 아니라, 커널 트릭(kernel trick)이라 불리는 (입력 자료의) 다차원 공간상으로의 맵핑(mapping) 기법을 사용하여 비선형분류도 효율적으로 수행한다.  

### 3. SVM 알고리즘

##### 예제1
iris 자료를 이용하여 SVM을 수행한다.
```{r ex1_1, warning=FALSE}
library(e1071)
data(iris)
```

```{r ex1_2}
svm.e1071 <- svm(Species ~., data=iris, type="C-classification",
                 kernel="radial", cost=10, gamma=0.1)
summary(svm.e1071)
```
plot()함수를 이용하여 그 결과에 대한 시각화할 수 있다.
```{r ex1_3}
plot(svm.e1071, iris, Petal.Width ~ Petal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))
# 나머지 변수의 지정된 값에서의 단면(2차원 그림)으로, (x: 서포트벡터, o: 자료점)임
```

```{r ex1_4}
plot(svm.e1071, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Petal.Width = 2.5, Petal.Length = 3))
```

```{r ex1_5}
pred <- predict(svm.e1071, iris, decision.values=TRUE)
(acc <- table(pred, iris$Species))
```
분류된 데이터를 실제 값과 비교해보면 setosa는 50개 모두 잘 분류되었고, versicolor은 50개 중 47개가 잘 분류되었으며 virginica는 50개 모두 잘 분류되었다.  

classAgreement() 함수를 통해 모형의 정확도를 확인할 수 있다.  
```{r ex1_6}
classAgreement(acc)
```
tune() 함수는 제공된 모수 영역에서 격자 탐색을 사용하여 통게적 방법의 초모수(hyperparameters)를 조율(tune)할 수 있다.  
이 함수는 최적의 모수를 제공해 주며, 동시에 여러 모수값에 대한 검정에 대한 자세한 결과를 제공해 준다.  
```{r ex1_7}
tuned <- tune.svm(Species~., data=iris,
                  gamma = 10^(-6:-1), cost = 10^(1:2))
# 6*2=12개 조합에서 모수 조율이 이루어짐
summary(tuned)
```

##### 예제2
iris 자료를 이용하여 SVM을 수행한다.
```{r ex2_1}
library(kernlab)
data(iris)
svm.kernlab <- ksvm(Species~., data=iris,
                    type = "C-bsvc", kernel = "rbfdot",
                    kpar = list(sigma=0.1), C=10, prob.model=TRUE)
svm.kernlab
```
plot() 함수를 이용하여 분류된 결과에 대한 각 변수별 분포를 상자그림의 형태로 나타낼 수 있다.  
```{r ex2_2}
fit <- fitted(svm.kernlab)
par(mfrow=c(2,2))
plot(fit, iris[,1], main="Sepal.Length")
plot(fit, iris[,2], main="Sepal.Width")
plot(fit, iris[,3], main="Petal.Length")
plot(fit, iris[,4], main="Petal.Width")
par(mfrow=c(1,1))
```
```{r ex2_3}
head(predict(svm.kernlab, iris, type="probabilities"))
predict(svm.kernlab, iris, type="response")
table(predict(svm.kernlab, iris), iris[,5])
```
predict() 함수를 통해 새로운 자료에 대한 분류(예측)를 수행할 수 있다.  
여기서는 모형구축에 사용된 자료르 재사용하여 분류를 수행하였다.  
그 결과 setosa와 virginica는 50개 모두, versicolor는 50개 중 47개가 제대로 분류되었다.  
  
##### 예제3
svm() 함수를 통해 SVR을 수행하고, 모수죠율(tuning)을 통해 성능을 제고한다.
```{r ex3_1}
# 분석용 자료 생성
x <- c(1:20)
y <- c(3,4,8,4,6,9,8,12,15,26,35,40,45,54,49,59,60,62,63,68)
data <- data.frame(x, y)
# SVR과의 비교를 위해 lm() 함수를 통해 단순회귀를 적합한다.
plot(data, pch=16)
model <- lm(y ~ x , data)
abline(model)
lm.error <- model$residuals   # same as data$Y - predicted Y
(lmRMSE <- sqrt(mean(lm.error^2)))
```
그 결과 RSME는 약 5.70을 얻었다.  
  
svm() 함수를 통해 SVR을 수행하고, 그 결과를 단순회귀결과와 함께 그린다.  
SVR의 RMSE가 약 3.16으로 단순회귀에 비해 감소되었음을 알 수 있다.
```{r ex3_2}
model <- svm(y ~ x, data)
pred.y <- predict(model, data)
plot(data, pch=16)
model <- lm(y ~ x , data)
abline(model)
points(data$x, pred.y, col="red", pch=4)
# 아래 그림에서 직선은 단순회귀, x는 SVR, ●은 자료점을 나타냄
error <- data$y - pred.y
(svmRMSE <- sqrt(mean(error^2)))
```
SVR의 성능을 높이기 위해 모델의 모수들을 최적화할 필요가 있다.  
적용된 svm() 함수는 디폴트로 epsilon-회귀(type=eps-regression 옵션)가 적용되었으며, epsilon값은 디폴트로 0.1이 사용되었다.  
과적합(overfitting)을 피하기 위해 cost 모수(cost=옵션)를 변경할 수 있다.  
이러한 모수의 선택과정을 초모수 최적화(hyperparameter optimization) 또는 모형선택(model selection)이라고 한다.  
  
모수 epsilon과 cost의 다양한 값에 대해, 격자탐색을 수행하여 최적의 모수를 찾는다.  
이 예제에서는 총 88개의 모형에 대해 훈련이 수행되었다. 그 결과 최적의 모수로 epsilon=0, cost=256이 선택되었다.  
```{r ex3_3}
tuneResult <- tune(svm, y~x, data=data,
                   ranges = list(epsilon=seq(0,1,0.1), cost=2^(2:9)))
print(tuneResult)
```
tune() 함수는 각 모형에 대한 MSE 값을 제공한다(제곱근을 취해 RMSE 값을 구할 수 있다).  
plot() 함수를 통해 RMSE 값을 시각화하면 다음과 같다.  
이 그림에서 색이 짙을수록 RMSE가 0에 가까우므로 더 나은 모형임을 의미한다.  
```{r ex3_4}
plot(tuneResult)
```
보다 정교한 모수조율(tuning)을 위해 보다 좁은 영역에서 격자탐색을 실시한다.  
위 그림에서 cost 모수의 영향은 크지 않으므로 그 값을 그대로 유지하였다.  
총 168개의 모형에 대해 훈련하였다.  
```{r ex3_5}
tuneResult2 <- tune(svm, y~x, data=data,
                   ranges=list(epsilon=seq(0,0.2,0.01), cost=2^(2:9)))
print(tuneResult2)
plot(tuneResult2)
```
위의 그림에서 진한 영역을 자세히 볼 수 있으며, 그 결과 epsilon=0.09, cost=128인 모형이 최소의 오차를 가지는 것을 알 수 있다.  
  
다행히도 최적의 모형을 분석자가 찾아낼 필요는 없다.  
R에서는 다음과 같이 매우 쉬운 방법으로 최적의 모형이 제공되며, 이를 통해 예측을 수행할 수 있다.  
```{r ex3_6}
tunedModel <- tuneResult2$best.model
tpred.y <-  predict(tunedModel, data)
error <- data$y - tpred.y
(tsvmRMSE <- sqrt(mean(error^2)))
```
위 결과에서 SE는 tsvmRMSE로 모형의 성능이 크게 개선되었음을 알 수 있다.  
  
모수에 대한 조율(tuning) 전과 후의 SVR을 적합한 결과를 그림으로 그려보면 다음과 같다.  
그림에서 푸른색(진한선)은 모수조율된 SVR 적합 결과이다.  
```{r ex3_7}
plot(data, pch=16)
points(data$x, pred.y, col="red", pch=4, type="b")
points(data$x, tpred.y, col="blue", pch=4, type="b")
```

